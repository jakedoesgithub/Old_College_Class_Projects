{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook is based off of geduldig's work with his TwitterAPI. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "from requests_oauthlib import OAuth1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from requests.exceptions import ConnectionError, ReadTimeout, SSLError\n",
    "from requests.packages.urllib3.exceptions import ReadTimeoutError, ProtocolError\n",
    "import TwitterAPI\n",
    "from TwitterAPI import *\n",
    "\n",
    "\n",
    "#Note to self: use credentials from twitter credentials text file to make this\n",
    "#worl. Remvoed them to save to github. Credentiasl are stored on computer in same location as this file.\n",
    "API_KEY = \"\"\n",
    "API_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "MONTH_ENDPOINT = \"\"\n",
    "FULL_ENDPOINT= \"\"\n",
    "VERIFY_CREDENTIALS_URL = ''\n",
    "\n",
    "url = MONTH_ENDPOINT\n",
    "auth = OAuth1(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRODUCT = '30day'\n",
    "LABEL = 'monthDev'\n",
    "QUERY = '(computer OR Computer) (science OR Science)  lang:en'\n",
    "TAG = '-is:retweet point_radius:[-92.018258,30.224502]'\n",
    "\n",
    "#this is to use to get the most recent tweets from the past 30 days within a mile of the center of LSU\n",
    "i = 1\n",
    "tweets = []\n",
    "class TweetMiner(object):\n",
    "    \n",
    "    \n",
    "    \"\"\"Uses next_tokens to paginate through Twitters REST API resource.\n",
    "    Supports Premium and Public resources. Also allows setting the number of pages to return.\n",
    "    Give a string object for the \"next_token\" parameter and\n",
    "    when this method ends it will set that object as the next key, which you can\n",
    "    use to continue your search from where you ended it. Supports searching back in time currently.\n",
    "    \n",
    "    :param api: An authenticated TwitterAPI object\n",
    "    :param resource: String with the resource path (ex. search/tweets)\n",
    "    :param params: Dictionary of resource parameters\n",
    "    :param next_token: Holds the next token when program ends. Set it as the toDate resource param to\n",
    "                        start your search from where you left off\n",
    "    :param num_pages: The number of responses get before shutting down. Each response is 100-500 tweets depending\n",
    "                        on resource used (basic, premium, etc). Leave blank to return as many results as possible\n",
    "    \"\"\"\n",
    "    def __init__(self, api, resource, params=None, next_token=None, num_pages= -1):\n",
    "        self.api = api\n",
    "        self.resource = resource\n",
    "        self.params = params\n",
    "        self.next_token = next_token\n",
    "        self.num_pages = num\n",
    "        \n",
    "    def begin_mining(self, wait=5):\n",
    "        \"\"\"Iterates through the response and yields the delicious tweets inside.\n",
    "        Loops until the required number of pages have been mined.\n",
    "        \n",
    "        :param wait: The amount of time (in seconds) to pause between requests.\n",
    "                        Values should be 5<=x<=60 depening on resource. Default 5.\n",
    "                        \n",
    "        :returns: JSON object containing statuses, errors or other info. \n",
    "        :raises: TwitterRequestError\n",
    "        \"\"\"\n",
    "        \n",
    "        #were working on this part when stopped. \n",
    "        #model after TwitterAPI and then use widgets to make an interactive jupyte notebooks version\n",
    "        #then make a normal version of it \n",
    "        time_elapsed = 0\n",
    "        if num_pages != -1:\n",
    "            this.params['toDate'] =  \n",
    "            \n",
    "    \n",
    "    start = time.time()\n",
    "    request_dict = {'query' : QUERY,\n",
    "                    'tag' : TAG}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #make request\n",
    "    r = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL),\n",
    "                    request_dict)\n",
    "    print (i)\n",
    "    i = i + 1\n",
    "\n",
    "    for item in iterate_request(r):\n",
    "        tweets.append(item)\n",
    "    \n",
    "    #get token\n",
    "    cursor = -1\n",
    "    r_json = r.json()\n",
    "    if 'next' in r_json:\n",
    "        cursor = r_json['next']\n",
    "        elapsed = time.time() - start\n",
    "        pause = wait - elapsed if elapsed < wait else 0\n",
    "        time.sleep(pause)\n",
    "        \n",
    "    #check for next page of tweets if token is valid, if token is -1 quit running\n",
    "    while cursor != -1 and i < 100:\n",
    "        start = time.time()\n",
    "        #add token to request dict\n",
    "        request_dict['next'] = cursor\n",
    "        \n",
    "        #make request\n",
    "        r = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL),\n",
    "                    request_dict)\n",
    "        print(i)\n",
    "        i = i + 1\n",
    "\n",
    "        #add items to tweet list\n",
    "        for item in iterate_request(r):\n",
    "            tweets.append(item)\n",
    "            \n",
    "        #get token\n",
    "        cursor = -1\n",
    "        r_json = r.json()\n",
    "        if 'next' in r_json:\n",
    "            cursor = r_json['next']\n",
    "            \n",
    "        #pause to ensure new request works\n",
    "        elapsed = time.time() - start\n",
    "        pause = wait - elapsed if elapsed < wait else 0\n",
    "        time.sleep(pause)\n",
    "        \n",
    "        \n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "#write results to file\n",
    "with open('30day_ullcsc_tweets.txt', 'w') as file:\n",
    "        file.write(json.dumps(tweets, indent=4))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
